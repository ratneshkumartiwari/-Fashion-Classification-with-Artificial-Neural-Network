# -*- coding: utf-8 -*-
"""m23mac011.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkO3xrsLLG5t7iFJMYZTUsjodc-scNLa
"""

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import random



class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.input_size = input_size
        self.hidden_layers = hidden_layers
        self.output_size = output_size
        self.weights, self.biases = self.initialize_weights()

    def initialize_weights(self):
        layers = [self.input_size] + self.hidden_layers + [self.output_size]
        weights = {}
        biases = {}
        np.random.seed(11)
        for i in range(1, len(layers)):
            weights[f'W{i}'] = np.random.randn(layers[i-1], layers[i])
            biases[f'b{i}'] = np.ones((1, layers[i]))
        return weights, biases

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exps = np.exp(x - np.max(x))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_propagation(self, X):
        activations = {'A0': X}
        for i in range(1, len(self.hidden_layers) + 2):
            activations[f'Z{i}'] = np.dot(activations[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']
            activations[f'A{i}'] = self.sigmoid(activations[f'Z{i}']) if i < len(self.hidden_layers) + 1 else self.softmax(activations[f'Z{i}'])
        return activations

    def backward_propagation(self, X, y, activations, learning_rate, reg_lambda = 0.01):
        m = X.shape[0]
        grads = {}
        activations['A0'] = X
        for i in reversed(range(1, len(self.hidden_layers) + 2)):
            if i == len(self.hidden_layers) + 1:
                # Assuming y_train contains categorical labels; convert to one-hot encoded vectors
                y_one_hot = np.zeros((m, self.output_size))
                y_one_hot[np.arange(m), y] = 1
                error = activations[f'A{i}'] - y_one_hot
            else:
                error = np.dot(grads[f'dA{i+1}'], self.weights[f'W{i+1}'].T) * (activations[f'A{i}'] * (1 - activations[f'A{i}']))

            # Regularization term for weights (excluding bias terms)
            regularization_term = (reg_lambda / m) * self.weights[f'W{i}']

            grads[f'dA{i}'] = error
            grads[f'dW{i}'] = (np.dot(activations[f'A{i-1}'].T, error) + regularization_term) / m
            grads[f'db{i}'] = np.sum(error, axis=0, keepdims=True) / m

        # Update weights and biases incorporating regularization
        for i in range(1, len(self.hidden_layers) + 2):
            self.weights[f'W{i}'] -= learning_rate * grads[f'dW{i}']
            self.biases[f'b{i}'] -= learning_rate * grads[f'db{i}']


    def train(self, X_train, y_train, epochs, learning_rate, batch_size):
        train_losses = []
        train_accuracies = []
        m_train = X_train.shape[0]


        for epoch in range(epochs):
            # Train on training data
            train_epoch_losses = []
            train_epoch_accuracies = []

            indices = np.arange(m_train)
            np.random.shuffle(indices)

            for i in range(0, m_train, batch_size):
                batch_indices = indices[i:i+batch_size]

                X_batch = X_train.iloc[batch_indices]
                y_batch = y_train.iloc[batch_indices]

                activations = self.forward_propagation(X_batch)
                loss = self.calculate_loss(y_batch, activations['A' + str(len(self.hidden_layers) + 1)])
                train_epoch_losses.append(loss)

                predictions = np.argmax(activations['A' + str(len(self.hidden_layers) + 1)], axis=1)
                accuracy = np.mean(predictions == y_batch)
                train_epoch_accuracies.append(accuracy)

                self.backward_propagation(X_batch, y_batch, activations, learning_rate)

            train_loss = np.mean(train_epoch_losses)
            train_accuracy = np.mean(train_epoch_accuracies)
            train_losses.append(train_loss)
            train_accuracies.append(train_accuracy)


        return train_losses, train_accuracies

    def calculate_loss(self, y_true, y_pred):
        m = y_true.shape[0]
        loss = -np.sum(np.log(y_pred[np.arange(m), y_true])) / m
        return loss

data = pd.read_csv("/content/drive/MyDrive/Copy of data.csv")



# Splitting data into train-test sets for different ratios
ratios = [0.7, 0.8, 0.9]
for ratio in ratios:
    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-ratio, random_state=42)
    # Shuffle the DataFrame using sample with frac=1 (which shuffles all rows)
    data = data.sample(frac=1).reset_index(drop=True)
    X = data.drop('label', axis=1)
    y = data['label']
    # Define the ratio for splitting (e.g., 70% train, 30% test)
    train_ratio = ratio
    test_ratio = 1 - train_ratio

    # Calculate the number of elements for train and test sets
    train_size = int(len(data) * train_ratio)
    test_size = len(data) - train_size

    # Manually split the dataset
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_test = X[-test_size:]
    y_test = y[-test_size:]

    input_size = X_train.shape[1]
    output_size = len(np.unique(y_train))
    hidden_layers = [128, 64, 32]
    epochs = 25
    learning_rate = 0.008
    batch_size = 23

    #creating a neural network model named nn
    nn = NeuralNetwork(input_size, hidden_layers, output_size)

    # Training the neural network
    train_losses, tarin_accuracies = nn.train(X_train, y_train, epochs, learning_rate, batch_size)

    # Plotting accuracy and loss per epoch
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.plot(range(epochs), tarin_accuracies, label='Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(epochs), train_losses, label='Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Testing the neural network
    activations = nn.forward_propagation(X_test)
    predictions = np.argmax(activations['A' + str(len(hidden_layers) + 1)], axis=1)
    accuracy = np.mean(predictions == y_test)

    # Generating confusion matrix
    cm = confusion_matrix(y_test, predictions)

    # Plotting confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

    #printing the train and test accuracy
    print("The accuracy of test set is ", accuracy * 100 , "%")
    print("The accuracy of train set is ", tarin_accuracies[-1] * 100 , "%")

    # Calculating trainable and non-trainable parameters
    trainable_params = sum(np.prod(w.shape) for w in nn.weights.values()) + sum(np.prod(b.shape) for b in nn.biases.values())
    non_trainable_params = input_size * output_size  # Input to output connections
    print(f"Trainable parameters: {trainable_params}")
    print(f"Non-trainable parameters: {non_trainable_params}")

from google.colab import drive
drive.mount('/content/drive')